\documentclass[../paper.tex]{subfiles}

% Document
\begin{document}
There were several issues we encountered during the project. 
We will discuss the most important ones in this section.
\subsection{Data}
The first issue we encountered was the data we used to train our model.
\subsubsection{Data Collection}
The issue we faced was that data found online is not always trustworthy.
One of the dataset we used before MNIST, was one that we found on kagle, stating that it was a dataset of the American Sign Language alphabet.
After looking further into the dataset, we quickly found out that the dataset did not only consist of the American Sign Language alphabet, but also of other alphabets.
This was a big issue, as the model would not be able to distinguish between the different alphabets.
\subsubsection{Data mapping}
Another issue is that the dataset were always mapped to the full alphabet, including the letters J and Z.
This creates extra number of classes that our model needs to account of, leading to less accuaracy.
We fixed this by remapping the classes to the alphabet excluding J and Z.
\subsubsection{Data creation}
Why not create our own data? As mentioned before we tried to create our own data, but this was not as easy as we thought.
One of the problems is that quality of images can have an impact on the model. 
For instance if the obtained dataset is not in the same quality/format as the webcam we use to test the model, the model will not be able to predict the letters correctly or accuratly.
Not only that, creation of a dataset or combining datasets can also result in a dataset that is not balanced.
This means that the model will be biased towards a certain enviroment.
\subsubsection{Data preprocessing}
One more issue we had, taht we realised with landmarks, 
is that the data had been cropped too much and would not be recongnized as a hand.
To solve this issue, we just disabled cropping for landmarking.
\subsection{Model}
Making a good model is not easy, and we encountered several issues when creating the model.
\subsubsection{Overfitting}
One issue we encountered was that the model was overfitting. 
The model would quicly overfit causing our model to only remember certain letters, for instance only A for all hand signs.
Fixing this issue was not easy, as we had to try different things to prevent overfitting.
\subsubsection{Layers}
Next issue, deciding the layers and how many layers our model should have is a difficult task.
You have to account for the complexity of the model, the number of classes, the size of the dataset, etc.
Our model was originally overcomplex and would train longer than needed. 
At one point it was not complex enough and would not be able to distinguish between the different letters.
Complexity can also decrease the training time, if the layers are chosen correctly.
So, overall choosing the right layers and the right number of layers is a difficult task.
Another issue that can occur with complex models, is that the model would stop too early and not learn anything.
\subsection{Training}
Training the model was also a difficult task.
\subsubsection{Hyperparameters}
As mentioned before, there are a few hyperparameters that we used when training the model: the learning rate, the batch size, the number of epochs, min delta, and patience.
Finding the right values for these hyperparameters was a fiddly task that required a lot of trial and error. 
If the learning rate was too low, the model would not learn anything
and would essentially flatline right at the start. 
If the learning rate was too high, the model would learn too quickly and the predictions would be all over the place or result in overfitting.
When tested, it would sometimes predict the same letter, no matter what the input was. 
Other times, it would predict a different letter every time.
The same problems occured with the batch size. 
If it was too low, the model would not have enough data to learn in each epoch and it would end up the same as if the learning rate was too low.

We started with a learning rate of $1e^{-3}$, but that was extremely high, so then we reduced it, 
tested different values and ended up training our model mostly with a learning rate in the range of $ 1e^{-6} - 1e^{-8}$.
For the batch size, we started with a batch size of 32, 
but most of the time we decreased it to 16, as it gave us better results.

The number of epochs was initially 20! but it quickly became apparent that this was not enough. 
Most of the time we ended up training the model with a maximum of 1000 epochs and even later on with 10k to 100k epochs.
Min delta and patience we did not change around much. 
Sometimes a higher min delta would give us better results, because the difference between the loss of the validation set and the training set was too big.
The min delta was set to 0.0001 for most of the training and the patience was set to 2 (in the end it was raised to 4).

As mentioned previously, the dataset that we used was not big. This caused the model to overfit quickly.
We tried to fix this by training the model in a reduced dataset of the first 6 letters of the alphabet, 
but this did not help much. 
It gave better results than the full 24-letter dataset, but it was still 
not detecting the letters correctly in "real life" situations.

\end{document}
